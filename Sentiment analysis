import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

# Step 1: Load and explore data
train_df = pd.read_csv('C:/Users/15988/Downloads/train.csv')
test_df = pd.read_csv('C:/Users/15988/Downloads/test.csv')

# Check dataset size
print("Training dataset size:", len(train_df))
print("Testing dataset size:", len(test_df))

# Sentiment distribution
print("\nTraining dataset sentiment distribution:")
print(train_df['Sentiment'].value_counts())
print("\nTesting dataset sentiment distribution:")
print(test_df['Sentiment'].value_counts())

# Check for missing values
print("\nMissing values in the training dataset:")
print(train_df.isnull().sum())
print("\nMissing values in the testing dataset:")
print(test_df.isnull().sum())
# use a smaller subset of the dataset
train_sample_size = 15000  # speed up running time
test_sample_size = 359     # adjust the value anytime

train_df = train_df.sample(train_sample_size, random_state=42).reset_index(drop=True)
test_df = test_df.sample(test_sample_size, random_state=42).reset_index(drop=True)

# Step 2: Text preprocessing
# nltk.download('stopwords')
# nltk.download('wordnet')
# nltk.download('punkt')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess(text):
    text = text.lower()
    text = re.sub(r'\w+:\/{2}[\d\w-]+(\.[\d\w-]+)*(?:(?:\/[^\s/]*))*', '', text)
    text = re.sub(r'\W', ' ', text)
    text = re.sub(r'\d', ' ', text)
    words = nltk.word_tokenize(text)
    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]
    text = ' '.join(words)
    return text

train_df['Text'] = train_df['Text'].apply(preprocess)
test_df['Text'] = test_df['Text'].apply(preprocess)

# Step 3: Linguistic feature extraction
vectorizer = TfidfVectorizer()
X_train = vectorizer.fit_transform(train_df['Text'])
y_train = train_df['Sentiment']

X_test = vectorizer.transform(test_df['Text'])
y_test = test_df['Sentiment']

# Step 4: Build sentiment classification models
scaler = StandardScaler(with_mean=False)
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

logistic_regression = LogisticRegression(max_iter=1000)
logistic_regression.fit(X_train_scaled, y_train)

svm = SVC()
svm.fit(X_train_scaled, y_train)

naive_bayes = MultinomialNB()
naive_bayes.fit(X_train, y_train)

random_forest = RandomForestClassifier()
random_forest.fit(X_train, y_train)

# Step 5: Model evaluation
models = {
    'Logistic Regression': logistic_regression,
    'SVM': svm,
    'Naive Bayes': naive_bayes,
    'Random Forest': random_forest
}

for name, model in models.items():
    y_pred = model.predict(X_test_scaled if name != 'Naive Bayes' else X_test)
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    print(f'{name}:')
    print(f'Accuracy: {accuracy:.4f}, F1: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\n')
