import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from gensim.models import Word2Vec

# Step 1: Load and explore data
train_df = pd.read_csv('C:/Users/15988/Downloads/train.csv')
test_df = pd.read_csv('C:/Users/15988/Downloads/test.csv')

# Check dataset size
print("Training dataset size:", len(train_df))
print("Testing dataset size:", len(test_df))

# Sentiment distribution
print("\nTraining dataset sentiment distribution:")
print(train_df['Sentiment'].value_counts())
print("\nTesting dataset sentiment distribution:")
print(test_df['Sentiment'].value_counts())

# Check for missing values
print("\nMissing values in the training dataset:")
print(train_df.isnull().sum())
print("\nMissing values in the testing dataset:")
print(test_df.isnull().sum())

# use a smaller subset of the dataset
train_sample_size = 15000  # speed up running time
test_sample_size = 359     # adjust the value anytime

train_df = train_df.sample(train_sample_size, random_state=42).reset_index(drop=True)
test_df = test_df.sample(test_sample_size, random_state=42).reset_index(drop=True)

# Step 2: Text preprocessing
# nltk.download('stopwords')
# nltk.download('wordnet')
# nltk.download('punkt')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess(text):
    text = text.lower()
    text = re.sub(r'\w+:\/{2}[\d\w-]+(\.[\d\w-]+)*(?:(?:\/[^\s/]*))*', '', text)
    text = re.sub(r'\W', ' ', text)
    text = re.sub(r'\d', ' ', text)
    words = nltk.word_tokenize(text)
    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]
    text = ' '.join(words)
    return text

train_df['Text'] = train_df['Text'].apply(preprocess)
test_df['Text'] = test_df['Text'].apply(preprocess)

# Step 3: Linguistic feature extraction
vectorizer = TfidfVectorizer()
X_train = vectorizer.fit_transform(train_df['Text'])
y_train = train_df['Sentiment']

X_test = vectorizer.transform(test_df['Text'])
y_test = test_df['Sentiment']

# Step 3.1: Bag-of-words feature extraction
bow_vectorizer = CountVectorizer()
X_train_bow = bow_vectorizer.fit_transform(train_df['Text'])
X_test_bow = bow_vectorizer.transform(test_df['Text'])

# Step 3.2: TF-IDF feature extraction
tfidf_vectorizer = TfidfVectorizer()
X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['Text'])
X_test_tfidf = tfidf_vectorizer.transform(test_df['Text'])

# Step 3.3: Word2Vec feature extraction
def tokenize(text):
    return nltk.word_tokenize(text)

train_tokenized = train_df['Text'].apply
train_tokenized = train_df['Text'].apply(tokenize)
test_tokenized = test_df['Text'].apply(tokenize)

word2vec = Word2Vec(train_tokenized, vector_size=100, window=5, min_count=1, workers=4)
word2vec.train(train_tokenized, total_examples=len(train_tokenized), epochs=10)


def get_word2vec_features(words, model):
    word_vectors = [model.wv[word] for word in words if word in model.wv]

    if not word_vectors:
        return np.zeros(model.vector_size)

    return np.mean(word_vectors, axis=0)


X_train_w2v = np.vstack(train_tokenized.apply(lambda x: get_word2vec_features(x, word2vec)))
X_test_w2v = np.vstack(test_tokenized.apply(lambda x: get_word2vec_features(x, word2vec)))

# Step 4: Build sentiment classification models
scaler_bow = StandardScaler(with_mean=False)
X_train_bow_scaled = scaler_bow.fit_transform(X_train_bow)
X_test_bow_scaled = scaler_bow.transform(X_test_bow)

scaler_tfidf = StandardScaler(with_mean=False)
X_train_tfidf_scaled = scaler_tfidf.fit_transform(X_train_tfidf)
X_test_tfidf_scaled = scaler_tfidf.transform(X_test_tfidf)

scaler_w2v = StandardScaler()
X_train_w2v_scaled = scaler_w2v.fit_transform(X_train_w2v)
X_test_w2v_scaled = scaler_w2v.transform(X_test_w2v)


# Step 5: Model evaluation
def evaluate_model(y_test, y_pred, model_name, feature_name):
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')

    print(f"Model: {model_name}, Feature: {feature_name}")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}\n")


# Initialize the classifiers
classifiers = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'SVM': SVC(),
    'Multinomial Naive Bayes': MultinomialNB(),
    'Random Forest': RandomForestClassifier()
}

# Dictionary of feature sets
feature_sets = {
    'Bag-of-Words': (X_train_bow_scaled, X_test_bow_scaled),
    'TF-IDF': (X_train_tfidf_scaled, X_test_tfidf_scaled),
    'Word2Vec': (X_train_w2v_scaled, X_test_w2v_scaled)
}

# Train and evaluate the models
for classifier_name, classifier in classifiers.items():
    for feature_name, (X_train_feature, X_test_feature) in feature_sets.items():
        if classifier_name == 'Multinomial Naive Bayes' and feature_name == 'Word2Vec':
            continue  # Skip this combination as it's incompatible

        classifier.fit(X_train_feature, y_train)
        y_pred = classifier.predict(X_test_feature)
        evaluate_model(y_test, y_pred, classifier_name, feature_name)
